{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:35:24.038730Z",
     "start_time": "2020-05-29T09:35:20.022441Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:38:31.899971Z",
     "start_time": "2020-05-29T09:38:31.889998Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import Model, models\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input ,Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:06.125705Z",
     "start_time": "2020-05-29T09:36:03.450865Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../archives/reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:08.957132Z",
     "start_time": "2020-05-29T09:36:06.253366Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', parser=False, entity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:09.008004Z",
     "start_time": "2020-05-29T09:36:08.991041Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(docs, nlp=nlp):\n",
    "    \"\"\"\n",
    "    Takes a pandas series or list of texts and returns the list with the text cleaned\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    for doc in nlp.pipe(docs):\n",
    "        words = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'PRON':\n",
    "                words.append(token.text)\n",
    "                continue\n",
    "            if token.pos_ == 'PART':\n",
    "                words.append(token.lemma_.lower())\n",
    "                continue\n",
    "            if token.is_alpha and token.lemma_ != '-PRON-':\n",
    "                words.append(token.lemma_.lower())\n",
    "        text.append(' '.join(map(str, words)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:09.104738Z",
     "start_time": "2020-05-29T09:36:09.043897Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"Function that remove punctuation from a text\"\"\"\n",
    "    return \"\".join([x for x in text if x not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:09.343098Z",
     "start_time": "2020-05-29T09:36:09.139642Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned'], data['sentiment'], \n",
    "                                                    random_state = 42, test_size =0.05, stratify= data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:36:40.412954Z",
     "start_time": "2020-05-29T09:36:09.391981Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "maxlen = 5000\n",
    "embedding_dims = 300\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token = '<UNK>')\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxlen = max([len(x) for x in list_tokenized_train])\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "x_train = sequence.pad_sequences(list_tokenized_train, padding = 'post', \n",
    "                                 truncating = 'post', maxlen=maxlen)\n",
    "\n",
    "x_test = sequence.pad_sequences(list_tokenized_test, padding = 'post', \n",
    "                                 truncating = 'post', maxlen=maxlen)\n",
    "\n",
    "num_words = len(word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:41:13.486277Z",
     "start_time": "2020-05-28T16:41:13.410469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a embedding using glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('../archives/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dims))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:55:49.310131Z",
     "start_time": "2020-05-29T08:55:49.286195Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_sizes=[3, 4, 5]\n",
    "class_num = 1\n",
    "last_activation='sigmoid' \n",
    "sequence_length = data.shape[1]\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "convs = []\n",
    "max_poolings = []\n",
    "\n",
    "\n",
    "for kernel_size in kernel_sizes:\n",
    "    convs.append(Conv1D(256, kernel_size, activation='relu'))\n",
    "    max_poolings.append(GlobalMaxPooling1D())\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],), dtype='int32')\n",
    "\n",
    "embedding = Embedding(max_features, embedding_dims, weights=[embedding_matrix], input_length=maxlen)(inputs)\n",
    "\n",
    "convs2 = []\n",
    "for i in range(len(kernel_sizes)):\n",
    "    c = convs[i](embedding)\n",
    "    c = max_poolings[i](c)\n",
    "    convs2.append(c)\n",
    "x = Concatenate()(convs2)\n",
    "output = Dense(class_num, activation=last_activation)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:35:04.267477Z",
     "start_time": "2020-05-28T16:34:24.390614Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:37:26.944862Z",
     "start_time": "2020-05-29T09:37:14.848221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcmor\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model('../archives/model_textcnn_imbd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:00:46.269007Z",
     "start_time": "2020-05-29T08:57:53.532930Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:00:52.740433Z",
     "start_time": "2020-05-29T09:00:52.516185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.93      0.92      1225\n",
      "        True       0.93      0.91      0.92      1275\n",
      "\n",
      "    accuracy                           0.92      2500\n",
      "   macro avg       0.92      0.92      0.92      2500\n",
      "weighted avg       0.92      0.92      0.92      2500\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1139,   86],\n",
       "       [ 111, 1164]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (result > 0.5)\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:40:56.080746Z",
     "start_time": "2020-05-29T09:40:56.075756Z"
    }
   },
   "outputs": [],
   "source": [
    "example = [\"Joaquin Phoenix gives a tour de force performance, fearless and stunning in its emotional depth and physicality. \\\n",
    "            It's impossible to talk about this without referencing Heath Ledger's Oscar-winning performance from The Dark Knight, \\\n",
    "            widely considered the definitive live-action portrayal of the Joker, so let's talk about it. \\\n",
    "            The fact is, everyone is going to be stunned by what Phoenix accomplishes, because it's what many thought impossible \\\n",
    "            a portrayal that matches and potentially exceeds that of The Dark Knight's Clown Prince of Crime\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:40:58.460377Z",
     "start_time": "2020-05-29T09:40:58.454392Z"
    }
   },
   "outputs": [],
   "source": [
    "example = remove_punctuation(example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:41:02.124570Z",
     "start_time": "2020-05-29T09:41:00.969661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joaquin Phoenix gives a tour de force performance fearless and stunning in its emotional depth and physicality             Its impossible to talk about this without referencing Heath Ledgers Oscarwinning performance from The Dark Knight             widely considered the definitive liveaction portrayal of the Joker so lets talk about it             The fact is everyone is going to be stunned by what Phoenix accomplishes because its what many thought impossible             a portrayal that matches and potentially exceeds that of The Dark Knights Clown Prince of Crime'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:41:33.632514Z",
     "start_time": "2020-05-29T09:41:33.567657Z"
    }
   },
   "outputs": [],
   "source": [
    "example = cleaning([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:41:43.572884Z",
     "start_time": "2020-05-29T09:41:43.565901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joaquin phoenix give a tour de force performance fearless and stunning in emotional depth and physicality impossible to talk about this without reference heath ledgers oscarwinne performance from the dark knight widely consider the definitive liveaction portrayal of the joker so let talk about it the fact be everyone be go to be stun by what phoenix accomplish because what many think impossible a portrayal that match and potentially exceed that of the dark knights clown prince of crime']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:41:58.028466Z",
     "start_time": "2020-05-29T09:41:58.019494Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer.texts_to_sequences(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:42:00.277449Z",
     "start_time": "2020-05-29T09:42:00.271465Z"
    }
   },
   "outputs": [],
   "source": [
    "example_test = sequence.pad_sequences(tokenized_example, padding = 'post', \n",
    "                                         truncating = 'post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:42:02.617186Z",
     "start_time": "2020-05-29T09:42:02.476564Z"
    }
   },
   "outputs": [],
   "source": [
    "example_result = model.predict(example_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:42:19.773279Z",
     "start_time": "2020-05-29T09:42:19.765300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9997079]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T09:42:05.097551Z",
     "start_time": "2020-05-29T09:42:05.080596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is positive\n"
     ]
    }
   ],
   "source": [
    "if example_result[0] > 0.5:\n",
    "    print('The review is positive')\n",
    "else:\n",
    "    print('The review is negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
